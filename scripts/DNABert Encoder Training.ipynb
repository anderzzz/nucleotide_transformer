{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNABERT Encoder Training\n",
    "This script trains the encoder of BERT to sequences of DNA nucleotides. This is an implementation of the DNABert method, see https://www.biorxiv.org/content/biorxiv/early/2020/09/19/2020.09.17.301879.full.pdf\n",
    "\n",
    "At a high-level, the method works as follows:\n",
    "* Bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Setup\n",
    "The github package is installed. This is followed by a simple test run of a toy example. If this fails, something failed with the import or is not setup right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/anderzzz/nucleotide_transformer.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMODULE_LIST = ['datacollators', 'io', 'utils']\n",
    "assert all([submodule in dir(biosequences) for submodule in SUBMODULE_LIST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from biosequences.utils import dna_nucleotide_alphabet, NucleotideVocabCreator, Phrasifier\n",
    "\n",
    "SEQ_STR = 'AATGCGT'\n",
    "IDS_SEQ_STR = [3,8,19,62,43,32,1]\n",
    "SEQ_BATCH = ['AATGCGT', 'GGGGT']\n",
    "IDS_SEQ_BATCH = [[3,8,19,62,43,32,1], [3,47,47,48,1]]\n",
    "\n",
    "dna_vocab = NucleotideVocabCreator(dna_nucleotide_alphabet, do_lower_case=True).generate(3)\n",
    "with open('tmp_test.txt', 'w') as fout:\n",
    "    dna_vocab.save(fout)\n",
    "phrasifier = Phrasifier(stride=1, word_length=3)\n",
    "tokenizer = BertTokenizer(vocab_file='tmp_test.txt', tokenize_chinese_chars=False)\n",
    "out = tokenizer(phrasifier(SEQ_STR))\n",
    "\n",
    "assert out.input_ids == IDS_SEQ_STR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Parameters\n",
    "The next section defines all runtime parameters that determine what encoder is trained on what data and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Arguments2DNABertTraining:\n",
    "    '''Runtime arguments for training of DNABert Encoder\n",
    "    \n",
    "    Args:\n",
    "        folder_seq_raw (str): From where to read the raw data of nucleotide sequence chunks. If `None` the\n",
    "            assumption is the processed nucleotide data is available in `folder_seq_sentence` already.\n",
    "        seq_raw_format (str): File format of raw sequence chunk data. Currently CSV, GenBank and Fasta are \n",
    "            possible. \n",
    "        seq_raw_file_patter (str): The query that returns all relevant files in the `folder_seq_raw`. In case\n",
    "            there is only one file, set this to that filename.\n",
    "        upper_lower (str): If to assume nucleotide characters to be all upper case or all lower case. This has\n",
    "            to be consistently used throughout, so both in the processing of raw data and the tokenization.\n",
    "        folder_seq_sentence (str): Folder for the nucleotide sequence sentence files; this is where the\n",
    "            raw sequence processing outputs its data files and where the dataset creator later reads from.\n",
    "        seq_sentence_prefix (str): File prefix to use for the plurality of sequence sentence files.\n",
    "        word_length_vocab (int): The number of nucleotide residues comprises a word.\n",
    "        stride (int): The stride to use as a nucleotide sequence is processed into a nucleotide sentence.\n",
    "        split_ratio_test (float): The ratio of data to turn into testing data.\n",
    "        split_ratio_validate (float): The ratio of data to turn into validation data.\n",
    "        shuffle (bool): If the data should be shuffled.\n",
    "        seed (int): Random seed for data shuffling.\n",
    "        vocab_file (str): Name of vocabulary file.\n",
    "        create_vocab (bool): If the vocabulary file should be created; if `False` the vocabulary file is \n",
    "            assumed to already be in `folder_seq_sentence`.\n",
    "        chunk_size (int): How many words to concatenate and include in a batch\n",
    "        masking_probability (float): The average ratio of masked words in the data; note that the masking is \n",
    "            done in chunks of at least length `word_length_vocab`.\n",
    "        bert_config_kwargs (dict): Keyword argument dictionary for the configuration of the BERT model, see \n",
    "            Huggingsface `BertConfig`.\n",
    "        folder_training_input (str): The folder where a PyTorch variant of the Bert model and its parameters\n",
    "            is stored and to be used as starting point; if `None`, the initial parameters are randomly\n",
    "            initialized; typically this folder is the output of a previous training `folder_training_output`.\n",
    "        folder_training_output (str): The folder where a PyTorch variant of the Bert model and its parameters\n",
    "            is stored during and after training.\n",
    "        training_kwargs (dict): Keyword argument dictionary for the training, other than the `output_dir`, see\n",
    "            Huggingsface `TrainingArguments`.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    folder_seq_raw : str = None\n",
    "    seq_raw_format : str = 'csv'\n",
    "    seq_raw_file_pattern : str = '*.csv'\n",
    "    upper_lower : str = 'upper'\n",
    "    folder_seq_sentence : str = None\n",
    "    seq_sentence_prefix : str = ''\n",
    "    word_length_vocab : int = 3\n",
    "    stride : int = 1\n",
    "    split_ratio_test : float = 0.05\n",
    "    split_ratio_validate : float = 0.05\n",
    "    shuffle : bool = True\n",
    "    seed : int = 42\n",
    "    vocab_file : str = 'vocab.txt'\n",
    "    create_vocab : bool = True\n",
    "    chunk_size : int = 1000\n",
    "    masking_probability : float = 0.15\n",
    "    bert_config_kwargs : Dict = {}\n",
    "    folder_training_input : str = None\n",
    "    folder_training_output : str = None\n",
    "    training_kwargs : Dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the runtime arguments in the instance of `Arguments2DNABertTraining` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments2DNABertTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.folder_seq_sentence is None:\n",
    "    raise ValueError('The folder for sequence sentence files required')\n",
    "\n",
    "if arg.upper_lower == 'upper':\n",
    "    do_upper_case = True\n",
    "    do_lower_case = False\n",
    "elif arg.upper_lower == 'lower':\n",
    "    do_upper_case = False\n",
    "    do_lower_case = True\n",
    "else:\n",
    "    raise ValueError('The vocabulary is either all upper or all lower, but `upper_lower` of invalid value: {}'.format(args.upper_lower))\n",
    "\n",
    "if arg.folder_training_output is None:\n",
    "    folder_training_output_ = folder_seq_sentence\n",
    "else:\n",
    "    folder_training_output_ = folder_training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helpers\n",
    "Before the data and training starts, do a few imports and helper functiond definitions for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "random.seed(args.seed)\n",
    "\n",
    "from transformers import BertForMaskedLM, BertConfig\n",
    "from transformers import BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_datase\n",
    "\n",
    "from biosequences.io import NucleotideSequenceProcessor\n",
    "from biosequences.utils import NucleotideVocabCreator, dna_nucleotide_alphabet, Phrasifier\n",
    "from biosequences.datacollators import DataCollatorDNAWithMasking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sequence_grouper(seqs, chunk_size):\n",
    "    concat_seq = {k : sum(seqs[k], []) for k in seqs.keys()}\n",
    "    total_length = len(concat_seq[list(seqs.keys())[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {\n",
    "        k : [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concat_seq.items()\n",
    "    }\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_metrics(eval_pred):\n",
    "    '''Custom metrics for evaluation step are done here.\n",
    "\n",
    "    Args:\n",
    "        eval_pred :\n",
    "\n",
    "    Returns:\n",
    "        custom_metrics (dict): Keys are the name of the custom metric, value the numberic value of said metric\n",
    "\n",
    "    '''\n",
    "    logits, labels = eval_pred\n",
    "    pass\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Raw Data, Chunk, Split and Tokenize\n",
    "All steps to prepare the data for the training follows\n",
    "\n",
    "Bla bla bla on how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DataCollatorDNAWithMasking',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'datacollator_dnabert']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not args.folder_seq_raw is None:\n",
    "    dataprocessor = NucleotideSequenceProcessor(source_directory=args.folder_seq_raw,\n",
    "                                                source_file_format=args.seq_raw_format,\n",
    "                                                source_directory_file_pattern=args.seq_raw_file_pattern)\n",
    "    phrasifier = Phrasifier(stride=args.stride,\n",
    "                            word_length=args.word_length_vocab,\n",
    "                            do_upper_case=do_upper_case,\n",
    "                            do_lower_case=do_lower_case)\n",
    "    dataprocessor.save_as_json(save_dir=args.folder_seq_sentence,\n",
    "                               save_prefix=args.seq_sentence_prefix,\n",
    "                               seq_transformer=phrasifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.create_vocab:\n",
    "    dna_vocab = NucleotideVocabCreator(alphabet=dna_nucleotide_alphabet,\n",
    "                                       do_lower_case=do_lower_case,\n",
    "                                       do_upper_case=do_upper_case).generate(args.word_length_vocab)\n",
    "    with open('{}/{}'.format(args.folder_seq_sentence, args.vocab_file), 'w') as fout:\n",
    "        dna_vocab.save(fout)\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file='{}/{}'.format(args.folder_seq_sentence, args.vocab_file), do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = Path(args.folder_seq_sentence).glob('{}*.json'.format(args.seq_sentence_prefix))\n",
    "json_files = ['{}'.format(x.resolve()) for x in json_files]\n",
    "len_train = round(len(json_files) * (1.0 - args.split_ratio_test - args.split_ratio_validate))\n",
    "if len_train <= 0:\n",
    "    raise ValueError('Split ratios for test and validate exceed 1.0, leaving nothing for training')\n",
    "len_test = round(len(json_files) * args.split_ratio_test)\n",
    "pp = list(range(len(json_files)))\n",
    "if shuffle:\n",
    "    random.shuffle(pp)\n",
    "json_files_split = {'train' : [json_files[k] for k in pp[:len_train]]}\n",
    "if len_test > 0:\n",
    "    json_files_split['test'] = [json_files[k] for k in pp[len_train:len_train + len_test]]\n",
    "if len(json_files) - len_train - len_test > 0:\n",
    "    json_files_split['validate'] = [json_files[k] for k in pp[len_train + len_test:]]\n",
    "json_files = json_files_split\n",
    "seq_dataset = load_dataset('json', data_files=json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = seq_dataset.map(\n",
    "    lambda x: tokenizer(x['seq']), batched=True, remove_columns=['seq', 'id', 'name', 'description']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_dataset.map(\n",
    "    _sequence_grouper,\n",
    "    batched=True,\n",
    "    fn_kwargs={'chunk_size' : chunk_size}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Data Collation, Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorDNAWithMasking(tokenizer=tokenizer,\n",
    "                                           mlm_probability=args.masking_probability,\n",
    "                                           word_length=args.word_length_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.folder_training_input is None:\n",
    "    config = BertConfig(vocab_size=tokenizer.vocab_size,\n",
    "                        **args.bert_config_kwargs)\n",
    "    model = BertForMaskedLM(config=config)\n",
    "else:\n",
    "    model = BertForMaskedLM.from_pretrained(args.folder_training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=folder_training_output_,\n",
    "    **args.training_kwargs\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=lm_dataset['train'],\n",
    "    eval_dataset=lm_dataset['validate'],\n",
    "    compute_metrics=_compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(output_dir=folder_training_output)\n",
    "print ('It is done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
